# CharLSTM

(Yet Another) implementation of a character-level recurrent neural network (RNN)
- 2-layer LSTM (Long Short Term Memory) model
- mini-batch learning using the rmsprop algorithm
- regularization using L1 and/or L2 norms
- implemented using theano; can run on CPU or GPU
- project under MIT License

## Goal
Learn to produce valid textual content from an input text file which is read
character by character.

## Content of the repository
- `charlstm.py`: main file, where the class `CharLSTM` is defined
- `train.py`: script for training the model based on input data
- `produce_text.py`: script for producing text based on previously learned model parameters
- `text_input/`: directory where input text files are stored
- `training_results/`: where information on and results of the training process are stored
- `text_output/`: directory that contains the text sequences generated by the model after learning

## Prerequisites
- theano 0.8.x (older versions of theano have not been tested); see http://deeplearning.net/software/theano/install.html

## Getting started
- First you should learn good parameters for the RNN model using data. This is handled by `train.py`, to be run as follows:  
  `python train.py [path to the input file] [name of the run]`  
  Default values for the arguments are:
  - `[path to the input file] = text_input/prideandprejudice.txt`
  - `[name of the run] = run0`  

  Hyperparameters are given as argument to the `prepare_input()` and `model_setup()` methods of the `CharLSTM` class; see `charlstm.py` for the various possibilities.

  Results are stored in the directory `training_results/`; by default parameters are saved in an npz file every 10 epochs and there is a .out file containing the training and validation costs (useful to
  plot loss function and check train/validation accuracy) as well as parameter/update ratios.

- Sequences can be generated based on parameters stored in an npz file using `produce_text.py` as  
  `python produce_text.py [path to the npz file] [path to the output file]`  
  Parameters for text generation are specified when calling `CharLSTM`'s `generate_text()` method (see `charlstm.py` for the various possibilities).

## More information on LSTM and character-level text generation
- Alex Graves' paper
  http://arxiv.org/abs/1308.0850
- Andrej Karpathy's blog post
  http://karpathy.github.io/2015/05/21/rnn-effectiveness/
- Christopher Olah's blog post
  http://colah.github.io/posts/2015-08-Understanding-LSTMs/
